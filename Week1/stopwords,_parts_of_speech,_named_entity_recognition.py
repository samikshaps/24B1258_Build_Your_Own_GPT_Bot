# -*- coding: utf-8 -*-
"""Stopwords, Parts of speech, named entity recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xkbh-7fV92v3jPvW7n2RRUcmoPiK9RcS
"""

import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('punkt_tab')
nltk.download('stopwords')

#lemmatizing and removing stop words and finally joining all the modified words to sentences
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer=WordNetLemmatizer()
for i in range(len(sentences)):
    words=nltk.word_tokenize(sentences[i])
    #stemming if the word is not a stopword
    words=[lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))]
    #convert words to lowercase since lemmatization does not convert to lowercase
    #adding post tag will give better results
    #converting all the list of words into sentences
    sentences[i]=' '.join(words)

print(sentences)

#finding post tags for all words including stopwords

paragraph= """Stemming and lemmatization are techniques used in natural language processing to reduce words to their base form. Stemming cuts off word endings to get a rough root, while lemmatization uses grammar and dictionaries to return a proper base word. For example, "running" becomes "run" in both cases, but stemming may turn "studies" into "studi", while lemmatization gives "study". The Porter Stemmer, one of the earliest algorithms, is fast but less accurate. The Snowball Stemmer, also called Porter2, is an improved version offering more consistency and support for multiple languages. Both are essential tools for text analysis and search engines."""

sentences = nltk.sent_tokenize(paragraph)

for sentence in sentences:
    words = nltk.word_tokenize(sentence)
    words = [word for word in words if word.lower() not in stopwords.words('english')]
    pos_tag = nltk.pos_tag(words)
    print(pos_tag)

print(nltk.pos_tag("Taj Mahal is a beautiful monument".split()))
#nltk.pos_tag() takes a list of strings
 -

import nltk
nltk.download('maxent_ne_chunker')
nltk.download('words')
sentence="The Eiffel Tower was built from 1887 - 1889 by French engineer Gustave Eiffel, whose company specialized in building metal frameworks and structures."
words=nltk.word_tokenize(sentence)
tags=nltk.pos_tag(words)
nltk.ne_chunk(tags).pprint()

